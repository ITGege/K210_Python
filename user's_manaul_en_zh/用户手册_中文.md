# 概述
## 1.作者简介
项目参与人与仓库维护者：李展博，北京航空航天大学自动化科学与电气工程学院硕士研究生
项目负责人：李大伟，北京航空航天大学无人系统研究院研究员(副教授)
项目负责人：杨炯，北京航空航天大学无人系统研究院工程师
联系方式：lizhanbo@buaa.edu.cn
## 2.项目简介
本K210项目为Vision_MAV项目的配套项目，Vision_MAV项目旨在设计并实现一个依托深度学习和图像处理技术的基于视觉的微型无人机系统，能够实现在无GPS环境下的自主视觉导航、目标检测与追踪，该项目由北航无人系统研究院李大伟副教授课题组创立并进行研究，并将在项目没有保密需求后进行开源。本仓库的K210项目是Vision_MAV的一个配套项目，基于[嘉楠科技公司](https://canaan-creative.com/ "嘉楠科技公司")生产的边缘AI芯片[K210](https://canaan-creative.com/product/kendryteai "K210")，来实现目标检测与追踪，为Vision_MAV项目提供一个可选的视觉解决方案。该项目基于一块[矽速科技公司](https://www.sipeed.com/ "矽速科技公司")生产的MAXI DOCK K210评估板，来验证K210芯片的AI计算能力。在本项目中，采用传统机器视觉方法实现了最大色块识别、二维码识别、Apriltag码识别、圆形识别，采用深度学习方法实现了人脸识别、人体识别、口罩识别等，并开发了K210和[Ardupilot](https://github.com/ArduPilot/ardupilot "Ardupilot")飞控固件之间的[MAVlink](https://github.com/ArduPilot/mavlink "MAVlink")通讯接口，来实时的将K210视觉模组解算出的目标位置信息发送给飞控来控制无人机运动。
## 3.边缘AI解决方案-K210芯片
随着人工智能技术的发展，深度学习已经被证明在目标检测、识别和跟踪领域相较于传统的计算机视觉方法有较大的优势。2012年以来，越来越多的基于各种深度神经网络模型的目标检测、分类、识别模型被提出，并不断刷新着神经网络的性能。通常情况下深度神经网络从训练到部署需要强大的算力和内存支持，这就对计算机硬件配置有了很高的要求，传统的移动设备无法满足这一要求，因此使得深度学习在移动端的部署受到了极大的限制。为了解决这一难题，硬件厂商巨头纷纷推出了体积小、功耗低但同时具有强大算力的移动计算平台，例如英伟达公司的J系列、华为海思351*系列等等。除此之外，近年来诞生了很多AI芯片初创公司，通过设计专用的神经网络加速芯片，来为边缘AI提供解决方案，例如寒武纪公司、矽速科技公司等。
本项目采用的K210平台就是矽速科技公司设计的一款RISC-V双核处理器，该处理器内部集成了一个神经网络加速器KPU，能够为神经网络运算提供1TOPS的算力，功耗仅为300mW。能够支持QVGA@60fps/VCA@30fps图像处理，支持YOLOv3/Mobilenetv2/TinyYOLOv2等神经网络模型，同时具有FPIOA、UART、GPIO、SPI、IIC、TIMER等外设。该芯片片上内存(SRAM)8MiB，分为6MiB的通用内存，2MiB的KPU专业内存，因此内存方面并不具有优势。在实际使用中，一个2MiB的神经网络模型就会包内存不足，分析一下主要的内存消耗：
* Maxipy固件：800K-1.9MiB
* 神经网络模型：1-3MiB
* 运行时中间变量若干

因此在使用这款AI芯片时，神经网络的压缩和量化以及优化是关键。该芯片的架构如下图所示。

该芯片可以采用C语言进行裸机开发，也可以采用FreeRTOS嵌入系统来做开发，当然还有更简单的第三种方式，即采用[Micropython](https://github.com/micropython/micropython "Micropython")来进行开发。在使用Micropython开发时，需要给芯片烧录[MaxiPy](https://github.com/sipeed/MaixPy "MaxiPy")固件，Maxipy是将Micropython一直到K210上的一个项目，在Maxipy固件中已经集成了全部由的[openmv](https://github.com/openmv/openmv "openmv")机器视觉库，通过使用Micropython可以极大地降低开发难度并减小开发周期。该芯片的开发支持可以参考Maxipy的[官方论坛](https://cn.bbs.sipeed.com/ "官方论坛")和[官方文档](https://cn.maixpy.sipeed.com/zh/ "官方文档")。
## 4.MAXI DOCK K210开发板资源介绍
本项目采用了一块[矽速科技公司](https://www.sipeed.com/ "矽速科技公司")生产的MAXI DOCK K210评估板，来验证K210芯片的AI计算能力。该开发板可以在矽速科技的官方淘宝网店[购买](https://item.taobao.com/item.htm?spm=a230r.7195193.1997079397.10.3dfd341cwqsO8h&id=591616120470&abbucket=10 "购买")。该开发板采用模块+底板的方式设计，板载Type-C接口和USB-UART电路，用户可以直接使用USB Type-C线连接电脑进行开发。同时具有24针DVP摄像头接口、24针LCD接口和MicroSD卡槽，并引出了所有的IO来方便用户扩展。在本项目中使用的摄像头为[ov2640](https://item.taobao.com/item.htm?spm=a1z10.3-c-s.w4002-21231188706.9.19915d54UXzDtq&id=584862125082 "ov2640")。开发板如下图所示。

# 一、初始配置与操作基础
本章将详细介绍如何配置这款芯片的开发环境以及相关工具软件的使用方法。
## 1.摄像头和液晶屏安装教程
参照USB口定位，确定摄像头和液晶屏的方向。所有程序都是基于下图的安装方向编写。如果是其他安装方式，每个脚本程序中最开头的注释了介绍了程序修改方法来进行适配。


## 2.Maxipy固件烧录教程
### (1)Maxipy简介
[Maxipy](https://github.com/sipeed/MaixPy "MaxiPy" "Maxipy")是一个将Micropython语言移植到K210芯片上的工程，该工程发行的固件名字就叫做Maxipy，Maxipy固件中集成了大量的机器视觉算法，并全部兼容openmv的算法库，然后将这个固件烧录到K210上之后就可以在该单片机上运行python，相对于传统的C语言单片机开发，难度极大降低、效率极大提高。
新买的K210开发板都已经刷好了默认固件，可以直接使用。但是建议刷到最新版固件，新版固件bug少。在连接好摄像头和液晶屏后即可上电测试。首先需要下载安装开发环境[Maxipy IDE](https://cn.dl.sipeed.com/MAIX/MaixPy/ide "Maxipy IDE")，下载之后双击exe安装包，根据个人喜好进行安装即可，无特殊要求，安装路径全为英文名是一个良好的习惯。然后使用USB连接开发板和电脑USB口，将开发板连接Maxipy IDE,点击IDE左下角连接，点击左上角文件，打开该K210项目仓库中的scripts文件夹中的hello_world.py，点击左下角三角，即可实现在线运行。上述已经实现了板子的运行，并将摄像头拍摄到的画面显示在液晶屏上。但是还是建议参考下节方法来为板子升级最新固件。
### (2)固件烧录方法
首先[下载最新MaxiPy固件](https://cn.dl.sipeed.com/MAIX/MaixPy/release/master "下载最新固件")，然后[下载固件烧录工具kflash_gui](https://cn.dl.sipeed.com/MAIX/tools/kflash_gui "下载固件烧录工具kflash_gui")。如果由于网络原因无法下载，可以使用该K210项目仓库中已有的MaxiPy固件和烧录工具，但不能保证是最新版，MaxiPy固件在MaxiPy子文件夹内，kflash_gui在kflash_gui子文件夹内。MaxiPy固件常用的有如下三种，可根据自己需要选择，建议在不适用openmv库时烧录最小固件，可以节约内存。
* maixpy_v*_no_lvgl.bin： MaixPy固件, 不带LVGL版本.(LVGL是嵌入式GUI框架, 写界面的时候需要用到)
* maixpy_v*_full.bin： 完整版的MaixPy固件(MicroPython + OpenMV API + lvgl )
* maixpy_v0.3.1_minimum.bin： MaixPy固件最小集合，不支持 MaixPy IDE， 不包含OpenMV的相关算法

得到MaxiPy固件和烧录工具kflash_gui后，点击进入kflash_gui子文件夹，该文件夹内是固件烧录工具kflash_gui，注意不要删除里边的任何文件，里边的kflash_gui.exe即为可运行的固件烧录程序。如下图所示。

双击kflash_gui.exe，即可打开gui界面，如下图所示。

固件烧录程序打开后，将K210开发板通过USB数据线连接电脑，正常情况下，USB插入电脑后，烧录程序会自动识别并自动填写相关配置。
如果没有自动识别和自动配置，可进行手动配置：
* 开发板选择对应的开发板，我们使用的是MAXI DOCK
* 下载到选择Flash
* 串口设置中的端口选项选择K210开发板连接电脑后对应的端口，可以通过设备管理器查看端口号
* 波特率为1500000
* 速度模式为低速模式

配置完成之后，点击打开文件按钮进行浏览到你存放Maxipy固件的文件夹，找到所要下载的固件。此时界面如下图所示。

之后单击下载按钮，即可开始烧录固件，此时只需等待固件烧录完成即可，烧录成功界面如下所示。

之后点击OK按钮即可，烧录成功后开发板会自动重启，到此已完成固件烧录，可以关闭固件烧录软件。
## 3.IDE使用教程
### (1)IDE安装
IDE的下载和安装前述已经说明，和普通软件安装过程无异。安装好之后，桌面会出现名字为MaxiPy IDE的应用程序，双击运行，界面如下所示。左下角有两个按钮，上边的链条形状为连接按钮，用来连接开发板和IDE，下边的三角按钮为运行按钮，用来启动脚本在线运行，在未连接开发板时链接按钮为绿色，运行按钮为灰色。

### (2)开发板和IDE连接
安装好IDE后，将K210开发板通过USB数据线和电脑连接，然后点击左下角的连接按钮，此时如果电脑连接了多个串口设备时，IDE会提示你选择要连接的串口，此时需要选择K210对应的串口，和固件烧录时的串口号是一致的。连接成功后，连接按钮编程红色，运行按钮编程绿色，如下图所示。此时点击红色连接按钮可断开连接。

### (3)在线运行脚本程序
成功连接开发板和IDE后，即可进行程序的编写和在线运行。可以点击IDE的文件按钮，选择新建脚本文件编写自己的程序，也可以点击打开文件按钮选择已有的脚本程序。这里选择已有的脚本程序，脚本程序存放scripts子文件夹内，通过IDE的文件按钮，浏览到脚本文件夹，全选所有的脚本，点击打开，打开上述脚本文件。打开后IDE如下图所示。

可以点击左上方的下三角按钮切换脚本，如下图所示。

此时可以修改程序，之后点击运行按钮即可在线运行脚本。如果提示out of memory可以重启开发板来释放内存。运行界面如下所示。

界面下方为log输出，可以看运行数据；右上方为图像窗口，可以展示摄像头画面。此时运行按钮变为红色，再次点击可停止运行。
### (4)脱机运行
上述程序运行是在线运行，只能连接IDE才可以运行。如果要实现脱机运行，需要将脚本程序烧写到K210开发板。烧写方法为：首先停止在线运行，然后点击IDE工具栏，点击工具选项，点击将打开的脚本保存到开发板的boot.py，然后点击是，等待完成即可。之后即可不依赖IDE，实现上电脱机运行。
## 4.神经网络模型使用方法
神经网络应用除了烧录上述对应的脚本程序之外，还要将训练好的神经网络模型放到K210开发板SD的根目录，训练过程后续会有讲解。例如在人体检测时使用了深度学习方法，其对应的网络模型存放在该仓库的model子文件夹内，名称为class.kmodel。将其拷贝到SD卡根目录即可。其他模型操作方法一样。.kmodel文件是K210支持的网络模型文件。
# 二、实际应用
本章介绍的应用涉及到的脚本程序均在该仓库的script子文件夹下。
## 1.坐标系
本项目程序使用到的所有程序坐标系如下图所示。不同程序中区别是图像大小，下图以160*120*为例，此时拍摄的图片大小为*160*120像素，坐标系原点位置在图像正中心，即像素点(80，60)是坐标系原点(0，0)，检测得到的目标位置中心点c坐标(x，y)为相对于坐标系原点(0，0)的坐标，当目标位于图像正中心时，输出的目标中心坐标应为(0，0)。

## 2.K210与Ardupilot通信-Mavlink协议实现
Mavlink协议是一种广泛使用的无人机系统之间的通信协议，可以实现飞控、外设、地面战等设备之间的信息交流，其源码和帧格式参见[MAVlink](https://github.com/ArduPilot/mavlink "MAVlink")。在本次项目中需要实现K210芯片和Ardupilot飞控固件之间的通信，因此需要在K210上实现Mavlink协议的驱动来完成对K210解算出的视觉数据的打包和发送，这样飞控才能正确理解数据。具体设计的Mavlink帧格式如下所示。
本实验中Mavlink帧数据结构共包含8个部分，从头开始依次为：
>0XFE，payload长度(共33个字节)，序列号，系统ID，组件ID，消息ID，payload，checksum

其中payload共包含11个数据，其数据类型依次为fffffffHBBB，共占用33字节。f: 对应于C语言中float型，4字节；H：对应于C语言中unsigned short型，2字节; B：对应于C语言中unsigned char型，1字节，payload具体内容如下：
> 	x    : 目标相对于图像坐标系原点x方向坐标
	y    : 目标相对于图像坐标系原点y方向坐标
	flag  : 标志位，检测到目标为1，没检测到目标为0
	4    ：默认为4，暂时未使用，无实际意义，备用
	5    ：默认为5，暂时未使用，无实际意义，备用
	100  ：默认为100，暂时未使用，无实际意义，备用
	0    ：默认为0，暂时未使用，无实际意义，备用
	31010：默认为31010，暂时未使用，无实际意义，备用
	0    ：默认为0，暂时未使用，无实际意义，备用
	0    ：默认为0，暂时未使用，无实际意义，备用
	0    ：默认为0，暂时未使用，无实际意义，备用

根据上述数据结构，将K210识别到的目标中心坐标作为x和y然后封装成数据结构进行发送，本项目编写与之匹配的打包和发送程序源码如下：
```python
# 设置MAVlink的几个字节的信息
MAV_system_id = 1
MAV_component_id = 1
packet_sequence = 0
MAV_OPTICAL_FLOW_message_id = 76
MAV_OPTICAL_FLOW_extra_crc = 152

#初始化串口
uart = UART(UART.UART1, 115200, read_buf_len=4096)

# 编写计算校验位的函数
def checksum(data, extra):
    output = 0xFFFF
    for i in range(len(data)):
        tmp = data[i] ^ (output & 0xFF)
        tmp = (tmp ^ (tmp << 4)) & 0xFF
        output = ((output >> 8) ^ (tmp << 8) ^ (tmp << 3) ^ (tmp >> 4)) & 0xFFFF
    tmp = extra ^ (output & 0xFF)
    tmp = (tmp ^ (tmp << 4)) & 0xFF
    output = ((output >> 8) ^ (tmp << 8) ^ (tmp << 3) ^ (tmp >> 4)) & 0xFFFF
    return output

# Mavlink协议打包
def send_optical_flow_packet(x, y,flag):
    global packet_sequence
    temp = struct.pack("<fffffffHBBB",x,y,flag,4,5,100,0,31010,0,0,0)#

    #print(len(temp))
    temp = struct.pack("<bbbbb33s",
                       33,
                       packet_sequence & 0xFF,
                       MAV_system_id,
                       MAV_component_id,
                       MAV_OPTICAL_FLOW_message_id,
                       temp)

    #print(len(temp))
    temp = struct.pack("<b38sh",
                       0xFE,
                       temp,
                       checksum(temp, MAV_OPTICAL_FLOW_extra_crc))

    #print(len(temp))

    print (struct.unpack("<bbbbbbfffffffHBBBh",temp))

    print([hex(x) for x in temp])

    packet_sequence += 1

    uart.write(temp)
    return temp
```

## 3.初次运行 Hello World
按照国际惯例，我们都会运行一下hello world程序来作为我们进行开发的开始，通过这个程序可以验证一下K210开发板的基本功能和外设是否正常，例如液晶屏和摄像头。该程序对应scripts文件夹下的hello_world.py，该程序将摄像头采集到的图像显示在LCD液晶屏上，并在IDE串口打印帧率，源码如下：
```python
import sensor, image, time, lcd

lcd.init(freq=15000000)
sensor.reset()                      # Reset and initialize the sensor. It will
sensor.set_vflip(1)
                                    # run automatically, call sensor.run(0) to stop
sensor.set_pixformat(sensor.RGB565) # Set pixel format to RGB565 (or GRAYSCALE)
sensor.set_framesize(sensor.QVGA)   # Set frame size to QVGA (320x240)
sensor.skip_frames(time = 2000)     # Wait for settings take effect.
clock = time.clock()                # Create a clock object to track the FPS.

while(True):
    clock.tick()                    # Update the FPS clock.
    img = sensor.snapshot()         # Take a picture and return the image.
    lcd.rotation(2)
    lcd.display(img)                # Display on LCD
    print(clock.fps())              # Note: MaixPy's Cam runs about half as fast when connected
                                    # to the IDE. The FPS should increase once disconnected.
```
## 4.机器视觉应用
硬件平台：K210开发板，软件平台：openmv机器视觉库。机器视觉类应用不能使用最小固件，因为最小固件里没有集成openmv机器视觉库。
### (1)最大色块识别
该应用对应scripts文件夹下的find_blob.py，其的目的是认为事先设定一个LAB色系颜色阈值，就可以选中一种颜色，程序会识别出图像中该颜色的最大连通区域，并给出区域中心坐标。该应用主要接住了openmv机器视觉库的find_blob库函数，然后采用任意一种排序算法对检测到的所有与预设颜色一致的色块面积大小进行排序，最终将面积最大的色块框起来就是代求的最大色块。该函数还未进行优化，优化后可大幅提高运行速度。源码如下：
```python
# 找最大色块 - By: lzbqr - 周一 5月 18 2020
'''
色块对象是由 image.find_blobs 返回的。
image.find_blobs(thresholds, invert=False, roi, x_stride=2, y_stride=1, area_threshold=10, pixels_threshold=10, merge=False,margin=0, threshold_cb=None, merge_cb=None)
查找图像中指定的色块。返回 image.blog 对象列表；
【thresholds】 必须是元组列表。 [(lo, hi), (lo, hi), ..., (lo, hi)] 定义你想追踪的颜色范围。 对于灰度图像，每个元组需要包含两个值 - 最小灰度值和最大灰度值。 仅考虑落在这些阈值之间的像素区域。 对于 RGB565 图像，每个元组需要有六个值(l_lo，l_hi，a_lo，a_hi，b_lo，b_hi) - 分别是 LAB L，A 和 B通道的最小值和最大值。
【area_threshold】若色块的边界框区域小于此参数值，则会被过滤掉；
【pixels_threshold】若色块的像素数量小于此参数值，则会被过滤掉；
【merge】若为 True,则合并所有没有被过滤的色块；
【margin】调整合并色块的边缘。
对于 RGB565 图像，每个元组需要有六个值(l_lo，l_hi，a_lo，a_hi，b_lo，b_hi)
分别是 LAB中 L，A 和 B 通道的最小值和最大值。
L的取值范围为0-100，a/b 的取值范围为-128到127。
'''
import sensor
import image
import lcd
import time
lcd.init()
sensor.reset(freq=24000000, set_regs=True, dual_buff=True)
sensor.set_pixformat(sensor.RGB565)
sensor.set_framesize(sensor.QVGA)
sensor.set_vflip(1)    #设置摄像头后置
sensor.run(1)

#红色阈值[0],绿色阈值[1],蓝色阈值[2]
rgb_thresholds =[
                (30, 100, 15, 127, 15, 127),
                (0, 80, -70, -10, -0, 30),
                (0, 30, 0, 64, -128, -20)]
while True:
    img=sensor.snapshot()
    blobs = img.find_blobs([rgb_thresholds[0]])
    a=[0,0,0,0,0,0,0,0]
    if blobs:
        for b in blobs:
            a[7]=b.area()
            if a[7]>a[6]:
                a[6]=a[7]
                a[0:4]=b.rect()
                a[4]=b.cx()
                a[5]=b.cy()
        img.draw_rectangle(a[0:4])
        img.draw_cross(a[4], a[5])
    lcd.rotation(2)
    lcd.display(img)

```
### (2)圆形识别与视频录制
程序会识别出图像中所有的圆形，并给出圆心坐标和圆半径。该应用主要接住了openmv机器视觉库的find_cycle函数。在该程序中还加入了录像功能，程序首先判断SD卡根目录是否有vedio文件夹，如果没有则创建一个vedio文件夹，然后将程序运行过程中的画面分程数个3秒的小视频按顺序命名存储到该目录下，便于后期调试，单独的视频录制脚本程序是video_record_v1.py。到程序源码如下：
```python
#创建视频录制对象,录制帧率25帧
if(Vedio_ON):
    vedio_flag=0
    dir_name=os.listdir()
    print(dir_name)
    for i_name in dir_name:
        if i_name == 'vedio':
            vedio_flag=1
    if vedio_flag==0:
        os.mkdir('vedio')
    dir_name=os.listdir()
    print(dir_name)
    i_frame=0
    j_video=1
    v_rec= video.open("/sd/vedio/capture1.avi", record=1, interval=200000, quality=50)

while(True):
    clock.tick()
    img = sensor.snapshot()
    img=img.mean(1)
    img=img.mean(1)
    img=img.mean(1)
    res  = img.find_circles(threshold = 3500, x_margin = 20, y_margin = 10, r_margin = 10,r_min = 2, r_max = 100, r_step = 2)
    res1 = img.find_circles(threshold = 3500, x_margin = 20, y_margin = 10, r_margin = 10,r_min = 2, r_max = 100, r_step = 2)
    fps =clock.fps()
    c_x=[]
    c_y=[]
    c_r=[]
    c_x1=[]
    c_y1=[]
    c_r1=[]
    if(LCD_ON):
        #显示帧率
        img.draw_string(2,2, ("%2.1ffps" %(fps)), color=(230,0,0), scale=2)
        #画坐标轴
        img.draw_arrow(int(img.width()/80),int(img.height()/2),int(img.width()-int(img.width()/80)),int(img.height()/2),150,5)
        img.draw_arrow(int(img.width()/2),int(img.height()-int(img.height()/80)),int(img.width()/2),int(img.height()/80),150,5)
        if res and res1:
            for i in res:
                c_x.append(i.x())
                c_y.append(i.y())
                c_r.append(i.r())

            for i in res1:
                c_x1.append(i.x())
                c_y1.append(i.y())
                c_r1.append(i.r())

            c_r_max=max(c_r)
            c_r_index=c_r.index(c_r_max)
            c_x_max=c_x[c_r_index]
            c_y_max=c_y[c_r_index]

            c_r_max1=max(c_r)
            c_r_index1=c_r.index(c_r_max1)
            c_x_max1=c_x[c_r_index1]
            c_y_max1=c_y[c_r_index1]

            if(abs(c_r_max1-c_r_max)<2) and (abs(c_x_max1-c_x_max)<2) and (abs(c_y_max1-c_y_max)<2):
                #框住目标
                img.draw_circle(int((c_x_max+c_x_max1)/2),int((c_y_max+c_y_max1)/2),int((c_r_max+c_r_max1)/2),color = (0, 255, 0),thickness = 2, fill = False)
                #计算码中心坐标，并画出中心点
                x=int((c_x_max+c_x_max1)/2)
                y=int((c_y_max+c_y_max1)/2)
                img.draw_cross(x,y,200,10,5)
                #显示中心点坐标
                img.draw_string(x+2,y+2, ("(%2.1f,%2.1f)" %((x-(img.width()/2)),((img.height()/2)-y))), color=(230,0,0), scale=2, mono_space=0)
                #给飞控发送mavlink帧
                send_optical_flow_packet((x-(img.width()/2)),((img.height()/2)-y),1)
        else:
            send_optical_flow_packet(0,0,0)
        lcd.display(img)
    else :
        print(fps)
        if res:
            for i in res:
                c_x.append(i.x())
                c_y.append(i.y())
                c_r.append(i.r())
            c_r_max=max(c_r)
            c_r_index=c_r.index(c_r_max)
            c_x_max=c_x[c_r_index]
            c_y_max=c_y[c_r_index]
            #计算码中心坐标
            x=int(c_x_max)
            y=int(c_y_max)
            #给飞控发送mavlink帧
            send_optical_flow_packet((x-(img.width()/2)),((img.height()/2)-y),1)
        else:
            send_optical_flow_packet(0,0,0)
    if(Vedio_ON):
        tim = time.ticks_ms()
        img_len = v_rec.record(img)
        print("record",j_video,i_frame,time.ticks_ms() - tim)
        i_frame += 1
        if i_frame == int(fps*3):
            print("finish:",j_video)
            j_video+=1
            v_rec.record_finish()
            i_frame=0
            v_rec = video.open("/sd/vedio/capture"+str(j_video)+".avi", record=1, interval=int(1000000/fps), quality=50)
    del c_x[:]
    del c_y[:]
    del c_r[:]
    del c_x1[:]
    del c_y1[:]
    del c_r1[:]
```
### (3)QRcode码与Apriltag码识别
QRcode码与Apriltag码识别的程序尽在调用openmv库函数处一行代码不同，因此放在一处说明。QRCode是我们日常生活中最常见的二维码格式，例如微信和支付宝扫码等，都是QRCode码。本程序调用openmv机器视觉库的find_QRcode库函数识别出图像中所有的QRcode码，并给出QRcode码的中心坐标和信息。Apriltag码是计算机视觉中广泛使用的一种定位标识码，本程序调用openmv机器视觉库的find_apritag库函数识别出图像中所有的Apriltag码，并给出Apriltag码的中心坐标和信息。本程序用于检测TAG16H5家族的Apriltag码，该家族共有29个不同的码，ID为0-28。检测程序将检测到的目标用绿色框圈主，并显示中心坐标。在屏幕上方还有两个参数，从左到右分别是帧率、ID。该结果对应的程序中输入图像大小为224*160，图片较大有一点卡顿，实验发现设置为160*120各方面最均衡。程序中修改输入图片大小的方法在程序最上边的注释中已经标记清楚。Apriltag码的识别程序源码如下，识别QRCode码时只需将第42行的库函数更换即可。
```python
# Apriltag_v3 - By: lzb - 周四 5月 28 2020
#第三版AprilTag识别程序，添加了ID显示，优化了坐标计算逻辑
'''
使用说明：
1.在第26行 sensor.set_windowing((224, 160))，中修改输入图片大小，宽高都应该为8的倍数
2.在第28行 sensor.set_vflip(1)，修改摄像头安装方式，1/0表示正面或反面
3.使用液晶屏时，在第116行，lcd.rotation(0)，修改液晶屏选装角度，0-3每增加1图像旋转90度。
4.第37行，修改LCD_off来使用或不适应液晶屏。0不使用，1使用。
'''
import sensor
import image
import lcd
import time
from machine import UART,Timer
from fpioa_manager import fm
import struct

clock = time.clock()
lcd.init()
sensor.reset(freq=24000000, set_regs=True, dual_buff=True)
sensor.set_pixformat(sensor.RGB565)
sensor.set_framesize(sensor.QVGA)

#此处可修改输入图像大小，经验证160*120大小时，最流畅。
sensor.set_windowing((224, 160))
#修改摄像头安装方式：1/0表示正面或反面
sensor.set_vflip(1)
sensor.run(1)

#注册与飞控通讯的串口
fm.register(6, fm.fpioa.UART1_RX, force=True)
fm.register(7, fm.fpioa.UART1_TX, force=True)

####lcd调试开关，打开方便调试，关掉略微增加帧率
LCD_off = 0
while True:
    clock.tick()
    img = sensor.snapshot()
    img=img.mean(1)
    img=img.mean(1)
    img=img.mean(1)
    res = img.find_apriltags(families=image.TAG16H5)
    fps =clock.fps()
    if(LCD_off):
        print(fps)
        if len(res) > 0:
          for i in res:
              #计算码中心坐标，并画出中心点
              x=int(i.x()+i.w()/2)
              y=int(i.y()+i.h()/2)
              #给飞控发送mavlink帧
              send_optical_flow_packet((x-(img.width()/2)),((img.height()/2)-y),1)
        else:
          send_optical_flow_packet(0,0,0)
    else :
        #显示帧率
        img.draw_string(2,2, ("%2.1ffps" %(fps)), color=(0,128,0), scale=2)
        #画坐标轴
        img.draw_arrow(int(img.width()/80),int(img.height()/2),int(img.width()-int(img.width()/80)),int(img.height()/2),150,5)
        img.draw_arrow(int(img.width()/2),int(img.height()-int(img.height()/80)),int(img.width()/2),int(img.height()/80),150,5)
        if len(res) > 0:
          for i in res:
              #输出码信息
              img.draw_string(int(img.width()/2),2, ("ID:%d"%(i.id())), color=(0,128,0), scale=2)
              #框住码
              img.draw_rectangle(i.rect(), color = (0, 255, 0),
                                  thickness = 2, fill = False)
              #计算码中心坐标，并画出中心点
              x=int(i.x()+i.w()/2)
              y=int(i.y()+i.h()/2)
              img.draw_cross(x,y,200,10,5)
              #显示中心点坐标
              img.draw_string(x+2,y+2, ("(%2.1f,%2.1f)" %((x-(img.width()/2)),((img.height()/2)-y))), color=(230,0,0), scale=2, mono_space=0)
              #给飞控发送mavlink帧
              send_optical_flow_packet((x-(img.width()/2)),((img.height()/2)-y),1)
        else:
          send_optical_flow_packet(0,0,0)
        lcd.rotation(0)
        lcd.display(img)

```
## 5.深度学习应用
硬件平台：K210开发板，软件平台：Tensorflow + tiny yolov2。在[矽速科技的官方论坛](https://www.maixhub.com/ "矽速科技的官方论坛")里提供了一些已经训练好的神经网络模型，可以直接将这些模型下载下来，然后按照1.4节介绍直接使用。除此之外也可以按照官方论坛里的训练方法或者自己的训练方法来训练自己的目标检测和分类模型，接下来将以官方提高的模型来进行测试，这些模型可以从官方论坛下载也可以在本仓库的model子文件夹内找到。
### (1)分类应用
使用官方提供的分类模型，可以识别包括人体在内的20种物体。加载神经网络模型class.kmodel,按照3.4节方法将模型文件夹里class.kmodel文件拷贝至开发板SD卡根目录。之后可以进行在线运行，将开发板连接Maxipy IDE,点击左下角连接，点击左上角文件，按照3.3.3节选择class.py脚本，点击左下角三角，即可在线运行。脱机运行时，进行完步骤三之后，停止在线运行，点击IDE工具栏，点击工具选项，点击将打开的脚本保存到开发板的boot.py，然后点击是，即可实现上电脱机运行。与该模型对应的脚本程序源码如下：
```python
#实验目的：使用class模型识别20种物体

import sensor,image,lcd,time
import KPU as kpu

#摄像头初始化
sensor.reset(freq=24000000, set_regs=True, dual_buff=True)
sensor.set_auto_gain(1)
sensor.set_pixformat(sensor.RGB565)
sensor.set_framesize(sensor.QVGA)
sensor.set_vflip(1)  #摄像头后置方式

lcd.init() #LCD初始化

clock = time.clock()

#模型分类，按照class顺序
classes = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']

#下面语句需要将模型（class.kfpkg）烧写到flash的 0x500000 位置
#task = kpu.load(0x400000)

#将模型放在SD卡中。
task = kpu.load("/sd/class.kmodel") #模型SD卡上

#网络参数
anchor = (1.889, 2.5245, 2.9465, 3.94056, 3.99987, 5.3658, 5.155437, 6.92275, 6.718375, 9.01025)

#初始化yolo2网络，识别可信概率为0.7（70%）
a = kpu.init_yolo2(task, 0.7, 0.3, 5, anchor)

while(True):

    clock.tick()

    img = sensor.snapshot()
    code = kpu.run_yolo2(task, img) #运行yolo2网络


    if code:
        for i in code:
            a=img.draw_rectangle(i.rect())

            img.draw_string(i.x(), i.y(), classes[i.classid()], color=(230,125,0), scale=2, mono_space=0)
            img.draw_string(i.x(), i.y()+12, '%f1.3'%i.value(), color=(230,150,0), scale=2, mono_space=0)
            fps =clock.fps()
            #显示帧率
            img.draw_string(2,2, ("%2.1ffps" %(fps)), color=(0,128,0), scale=2)
            lcd.rotation(2)
            a = lcd.display(img)

    else:
        fps =clock.fps()
        #显示帧率
        img.draw_string(2,2, ("%2.1ffps" %(fps)), color=(0,128,0), scale=2)
        lcd.rotation(2)
        a = lcd.display(img)

    #print(clock.fps())#打印FPS
```
### (2)人体检测
加载人体识别神经网络模型class.kmodel,按照3.4节方法将模型文件夹里class.kmodel文件拷贝至开发板SD卡根目录。在线运行，将开发板连接maxipy IDE,点击左下角连接，点击左上角文件，按照第一章第3节第(3)小节选择human_detect_v1.py脚本，点击左下角三角，即可在线运行。脱机运行，进行完步骤三之后，停止在线运行，点击IDE工具栏，点击工具选项，点击将打开的脚本保存到开发板的boot.py，然后点击是，即可实现上电脱机运行。该程序可以检测出程序里的人体，并输出人体坐标。程序源码如下：
```python
# Human_detect_v1 - By: lzbqr - 周四 5月 28 2020

#物品检测,可以检测包含人体在内的20种物体
'''
使用说明：
1.在第22行 sensor.set_vflip(1)，修改摄像头安装方式，1/0表示正面或反面
2.使用液晶屏时，在第151行，lcd.rotation(0)，修改液晶屏选装角度，0-3每增加1图像旋转90度。
3.第34行，修改LCD_off来使用或不适应液晶屏。0不使用，1使用。
'''

import sensor,image,lcd,time
import KPU as kpu
from machine import UART,Timer
from fpioa_manager import fm
import struct
#摄像头初始化
sensor.reset(freq=24000000, set_regs=True, dual_buff=True)
sensor.set_pixformat(sensor.RGB565)
sensor.set_framesize(sensor.QVGA)

#修改摄像头安装方式：1/0表示正面或反面
sensor.set_vflip(1)
sensor.run(1)

lcd.init() #LCD初始化
clock = time.clock()

#注册与飞控通讯的串口
fm.register(6, fm.fpioa.UART1_RX, force=True)
fm.register(7, fm.fpioa.UART1_TX, force=True)


####lcd调试开关，打开方便调试，关掉略微增加帧率
LCD_off = 0

#模型分类，按照class顺序
classes = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']

#将模型放在SD卡中。
task = kpu.load("/sd/class.kmodel") #模型SD卡上

#网络参数
anchor = (1.889, 2.5245, 2.9465, 3.94056, 3.99987, 5.3658, 5.155437, 6.92275, 6.718375, 9.01025)

#初始化yolo2网络，识别可信概率为0.7（70%）
a = kpu.init_yolo2(task, 0.7, 0.3, 5, anchor)

while(True):
    clock.tick()
    img = sensor.snapshot()

    res = kpu.run_yolo2(task, img) #运行yolo2网络
    fps =clock.fps()
    if(LCD_off):
        print(fps)
        if res:
            for i in res:
                a=classes[i.classid()]
                if (a=='person'):
                    #计算码中心坐标，并画出中心点
                    x=int(i.x()+i.w()/2)
                    y=int(i.y()+i.h()/2)
                    #给飞控发送mavlink帧
                    send_optical_flow_packet((x-(img.width()/2)),((img.height()/2)-y),1)
                else:
                    send_optical_flow_packet(0,0,0)
        else:
            send_optical_flow_packet(0,0,0)
    else :
        #显示帧率
        img.draw_string(2,2, ("%2.1ffps" %(fps)), color=(230,0,0), scale=2)
        #画坐标轴
        img.draw_arrow(int(img.width()/80),int(img.height()/2),int(img.width()-int(img.width()/80)),int(img.height()/2),150,5)
        img.draw_arrow(int(img.width()/2),int(img.height()-int(img.height()/80)),int(img.width()/2),int(img.height()/80),150,5)
        if res:
            for i in res:
                a=classes[i.classid()]
                if (a=='person'):
                    #框住目标
                    img.draw_rectangle(i.rect(), color = (0, 255, 0),thickness = 2, fill = False)
                    #计算码中心坐标，并画出中心点
                    x=int(i.x()+i.w()/2)
                    y=int(i.y()+i.h()/2)
                    img.draw_cross(x,y,200,10,5)
                    #显示中心点坐标
                    img.draw_string(x+2,y+2, ("(%2.1f,%2.1f)" %((x-(img.width()/2)),((img.height()/2)-y))), color=(230,0,0), scale=2, mono_space=0)
                    #显示标签和置信度
                    img.draw_string(2, 15, classes[i.classid()], color=(230,0,0), scale=2, mono_space=0)
                    img.draw_string(2, 30, "%2.3f"%i.value(), color=(230,0,0), scale=2, mono_space=0)
                    #给飞控发送mavlink帧
                    send_optical_flow_packet((x-(img.width()/2)),((img.height()/2)-y),1)
                else:
                    send_optical_flow_packet(0,0,0)
        else:
            send_optical_flow_packet(0,0,0)
        lcd.rotation(0)
        lcd.display(img)


```
运行结果说明：程序会检测出图片中的所有人体，并于绿色方框圈主目标，同时在绿色方框中心绘制红色十字叉，并显示十字叉中心坐标。在屏幕左上角还有三个参数，从上到下分别为：帧率、类别、置信度。

### (3)人脸识别
同样的，将face.kmodel模型添加至K210的SD卡根目录，然后运行faec_dect.py脚本，程序会识别并框住图片中的所有人脸，并输出人脸的中心坐标，程序源码如下：
```python
#实验名称：人脸检测
#翻译和注释：01Studio
#参考链接：http://blog.sipeed.com/p/675.html

import sensor,lcd,time
import KPU as kpu

#设置摄像头
sensor.reset(freq=24000000, set_regs=True, dual_buff=True)
sensor.set_auto_gain(1)
sensor.set_pixformat(sensor.RGB565)
sensor.set_framesize(sensor.QVGA)
sensor.set_vflip(1)    #设置摄像头后置

lcd.init() #LCD初始化

clock = time.clock()

#task = kpu.load(0x300000) #需要将模型（face.kfpkg）烧写到flash的 0x300000 位置
task = kpu.load("/sd/face.kmodel") #模型SD卡上

#模型描参数
anchor = (1.889, 2.5245, 2.9465, 3.94056, 3.99987, 5.3658, 5.155437, 6.92275, 6.718375, 9.01025)

#初始化yolo2网络
a = kpu.init_yolo2(task, 0.5, 0.3, 5, anchor)

while(True):
    clock.tick()
    img = sensor.snapshot()
    code = kpu.run_yolo2(task, img) #运行yolo2网络

    #识别到人脸就画矩形表示
    if code:
        for i in code:
            print(i)
            b = img.draw_rectangle(i.rect())
    fps =clock.fps()
    #显示帧率
    img.draw_string(2,2, ("%2.1ffps" %(fps)), color=(0,128,0), scale=2)
    #LCD显示
    lcd.rotation(2)
    lcd.display(img)

    #print(clock.fps())   #打印FPS

```
## 6.综合应用
### (1)特定颜色衣服的人体检测
该应用对应脚本human_detect_v3.py，使用时还需要将class.kmodel模型文件拷贝到SD卡根目录。该程序将人体检测、最大色块检测和视频录制以及和飞控通信的Mavlink功能集成到一个程序中去，能够实现检测视频中穿特定颜色衣服的人体目标，并实时输出目标中心的坐标，并通过Mavlink协议将中心坐标发送给飞控，可以实现实现无人机跟踪特定的目标，并将整个过程的画面保存到S卡根目录下的vedio文件见内。程序源码如下。
```python
# Human_detect_v3 - By: lzbqr - 周一 6月 22 2020

#本次更新加入了识别特定人的功能，只识别穿蓝色衣服的人
#录像功能,录像存储在SD卡根目录下的vedio文件夹内，由众多连续的小视频组成，每个小视频时长3秒
#物品检测,可以检测包含人体在内的20种物体
'''
使用说明：
1.在第24行 sensor.set_vflip(1)，修改摄像头安装方式，1/0表示正面或反面
2.使用液晶屏时，在第27行，lcd.rotation(0)，修改液晶屏选装角度，0-3每增加1图像旋转90度。
3.第33行，修改LCD_ON来使用或不适应液晶屏。0不使用，1使用。
4.第35行，修改Vedio_ON来进行录像开关，1开启录像，0关闭录像
'''

import sensor,image,lcd,time,video,os
import KPU as kpu
from machine import UART,Timer
from fpioa_manager import fm
import struct

#摄像头初始化
sensor.reset(freq=24000000, set_regs=True, dual_buff=True)
sensor.set_pixformat(sensor.RGB565)
sensor.set_framesize(sensor.QVGA)

#修改摄像头安装方式：1/0表示正面或反面
sensor.set_vflip(1)
sensor.run(1)
lcd.init() #LCD初始化
lcd.rotation(2)

clock = time.clock()

#调试开关，打开方便调试，关掉略微增加帧率
#lcd显示开关，1使用显示，0不使用LCD
LCD_ON = 1
#录像开关，1开启录像，0关闭录像
Vedio_ON = 1

#注册与飞控通讯的串口
fm.register(6, fm.fpioa.UART1_RX, force=True)
fm.register(7, fm.fpioa.UART1_TX, force=True)

##########################################编写有关Mavlink协议有关的代码#################################

# 设置MAVlink的几个字节的信息
MAV_system_id = 1
MAV_component_id = 1
packet_sequence = 0
MAV_OPTICAL_FLOW_message_id = 76
MAV_OPTICAL_FLOW_extra_crc = 152

#初始化串口
uart = UART(UART.UART1, 115200, read_buf_len=4096)

# 编写计算校验位的函数
def checksum(data, extra):
    output = 0xFFFF
    for i in range(len(data)):
        tmp = data[i] ^ (output & 0xFF)
        tmp = (tmp ^ (tmp << 4)) & 0xFF
        output = ((output >> 8) ^ (tmp << 8) ^ (tmp << 3) ^ (tmp >> 4)) & 0xFFFF
    tmp = extra ^ (output & 0xFF)
    tmp = (tmp ^ (tmp << 4)) & 0xFF
    output = ((output >> 8) ^ (tmp << 8) ^ (tmp << 3) ^ (tmp >> 4)) & 0xFFFF
    return output

# Mavlink协议打包
def send_optical_flow_packet(x, y,flag):
    global packet_sequence
    temp = struct.pack("<fffffffHBBB",x,y,flag,4,5,100,0,31010,0,0,0)#

    #print(len(temp))
    temp = struct.pack("<bbbbb33s",
                       33,
                       packet_sequence & 0xFF,
                       MAV_system_id,
                       MAV_component_id,
                       MAV_OPTICAL_FLOW_message_id,
                       temp)

    #print(len(temp))
    temp = struct.pack("<b38sh",
                       0xFE,
                       temp,
                       checksum(temp, MAV_OPTICAL_FLOW_extra_crc))

    #print(len(temp))

    print (struct.unpack("<bbbbbbfffffffHBBBh",temp))

    print([hex(x) for x in temp])

    packet_sequence += 1

    uart.write(temp)
    return temp

#模型分类，按照class顺序
classes = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']

#将模型放在SD卡中。
task = kpu.load("/sd/class.kmodel") #模型SD卡上

#网络参数
anchor = (1.889, 2.5245, 2.9465, 3.94056, 3.99987, 5.3658, 5.155437, 6.92275, 6.718375, 9.01025)

#初始化yolo2网络，识别可信概率为0.7（70%）
a = kpu.init_yolo2(task, 0.7, 0.3, 5, anchor)

#创建视频录制对象,录制帧率25帧
if(Vedio_ON):
    vedio_flag=0
    dir_name=os.listdir()
    print(dir_name)
    for i_name in dir_name:
        if i_name == 'vedio':
            vedio_flag=1
    if vedio_flag==0:
        os.mkdir('vedio')
    dir_name=os.listdir()
    print(dir_name)
    i_frame=0
    j_video=1
    v_rec= video.open("/sd/vedio/capture1.avi", record=1, interval=40000, quality=50)

#红色阈值[0],绿色阈值[1],蓝色阈值[2]
rgb_thresholds =[
                (0, 100, 39, 127, 25, 127),
                (0, 80, -70, -10, -0, 30),
                (43, 0, -128, 127, -128, -28)]

while(True):
    clock.tick()
    img = sensor.snapshot()
    res = kpu.run_yolo2(task, img) #运行yolo2网络
    blobs = img.find_blobs([rgb_thresholds[2]])
    a=[0,0,0,0,0,0,0,0]
    if blobs:
        for b in blobs:
            a[7]=b.area()
            if a[7]>a[6]:
                a[6]=a[7]
                a[0:4]=b.rect()
                a[4]=b.cx()
                a[5]=b.cy()
        img.draw_rectangle(a[0:4])
        img.draw_cross(a[4], a[5])
    fps =clock.fps()
    if(LCD_ON):
        #显示帧率
        img.draw_string(2,2, ("%2.1ffps" %(fps)), color=(230,0,0), scale=2)
        #画坐标轴
        img.draw_arrow(int(img.width()/80),int(img.height()/2),int(img.width()-int(img.width()/80)),int(img.height()/2),150,5)
        img.draw_arrow(int(img.width()/2),int(img.height()-int(img.height()/80)),int(img.width()/2),int(img.height()/80),150,5)
        if res:
            for i in res:
                class_type=classes[i.classid()]
                if (class_type=='person'):
                    #计算码中心坐标
                    x=int(i.x()+i.w()/2)
                    y=int(i.y()+i.h()/2)
                    if (abs(x-int(a[4]))<20) and (abs(y-int(a[5]))<30):
                        #框住目标
                        img.draw_rectangle(i.rect(), color = (0, 255, 0),thickness = 2, fill = False)
                        img.draw_cross(x,y,200,10,5)
                        #显示中心点坐标
                        img.draw_string(x+2,y+2, ("(%2.1f,%2.1f)" %((x-(img.width()/2)),((img.height()/2)-y))), color=(230,0,0), scale=2, mono_space=0)
                        #显示标签和置信度
                        img.draw_string(2, 15, classes[i.classid()], color=(230,0,0), scale=2, mono_space=0)
                        img.draw_string(2, 30, "%2.3f"%i.value(), color=(230,0,0), scale=2, mono_space=0)
                        #给飞控发送mavlink帧
                        send_optical_flow_packet((x-(img.width()/2)),((img.height()/2)-y),1)
                    else:
                        send_optical_flow_packet(0,0,0)
                else:
                    send_optical_flow_packet(0,0,0)
        else:
            send_optical_flow_packet(0,0,0)
        lcd.display(img)
    else :
        print(fps)
        if res:
            for i in res:
                a=classes[i.classid()]
                if (a=='person'):
                    #计算码中心坐标，并画出中心点
                    x=int(i.x()+i.w()/2)
                    y=int(i.y()+i.h()/2)
                    #给飞控发送mavlink帧
                    send_optical_flow_packet((x-(img.width()/2)),((img.height()/2)-y),1)
                else:
                    send_optical_flow_packet(0,0,0)
        else:
            send_optical_flow_packet(0,0,0)
    if(Vedio_ON):
        tim = time.ticks_ms()
        img_len = v_rec.record(img)
        print("record",j_video,i_frame,time.ticks_ms() - tim)
        i_frame += 1
        if i_frame == 75:
            print("finish:",j_video)
            j_video+=1
            v_rec.record_finish()
            i_frame=0
            v_rec = video.open("/sd/vedio/capture"+str(j_video)+".avi", record=1, interval=40000, quality=50)
    del a[:]

```
# 三、总结
本手册主要介绍了一下从拿到一个K210评估板到具体应用的环境配置、脚本编写等，并最终实现了相应的功能。从运行结果来看，基于yolov2-Tiny的神经网络应用可以实现30FPS的实时检测帧率，而基于传统的机器视觉的应用帧率通常在10FPS一下，因此K210的神经网络运算性能还是比较优秀，能够适合各种对于体积、重量和功耗要求极为严格的边缘计算场所或者移动计算场所，例如无人机等。但是在使用过程中也同时发现了其他的问题，例如K210芯片的双核CPU处理速度有限，因此必须尽可能降低图像的分辨率和尺寸，使得画质会影响。另一方面K210的内存相对不足，只有8MiB的RAM，这就对模型的大小提出了极为严格的要求，因此在内存方面具有一定的劣势。
总体来看，最嘉楠科技的第一款AI芯片，还是取得了不错的成绩，希望后续的系列AI芯片能够针对性的大幅度提升，也希望国产AI芯片越做越好。
# 四、致谢
在近2个月的项目过程中，感谢导师李大伟副教授在各方面都指导和支持，也感谢北航无人系统研究院杨炯工程师的技术支持，最终完成了对该AI芯片的个性评估，为课题组的vision_MAV微型视觉无人机项目提供了一个可选的视觉解决方案，并最终完成了基于二次开发Ardupilot飞控的微型无人机的搭建和试飞，实现了无人机和K210视觉系统的联调，能够让无人机跟踪特定颜色衣着人体，取得了良好的实验结果，为后续其他视觉解决方案的开发奠定了基础。